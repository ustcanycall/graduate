@book{lamport94,
    author    = "Leslie Lamport",
    title     = "{\LaTeX: A Document Preparation System}",
    year      = "1994",
    publisher = "Addison-Wesley",
    edition = "2nd",
    address   = "Reading, Massachusetts"
}

@inproceedings{nicepaper1,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2",
  title = "A Very Nice Paper To Cite",
  year = "2014",
  booktitle = "Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture"
}

@inproceedings{nicepaper2,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2 and Firstname3 Lastname3",
  title = "Another Very Nice Paper to Cite",
  year = "2012",
  booktitle = "Proceedings of the 45th Annual IEEE/ACM International Symposium on Microarchitecture"
}

@inproceedings{nicepaper3,
  author = "Firstname1 Lastname1 and Firstname2 Lastname2 and Firstname3 Lastname3 and Firstname4 Lastname4 and Firstname5 Lastname5",
  title = "Yet Another Very Nice Paper To Cite, With Many Author Names All Spelled Out",
  year = "2011",
  booktitle = "Proceedings of the 38th Annual International Symposium on Computer Architecture"
}

@inproceedings{mnih2012learning,
  title={Learning to label aerial images from noisy data},
  author={Mnih, Volodymyr and Hinton, Geoffrey E},
  booktitle={Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
  pages={567--574},
  year={2012}
}



%%%%%%%%%%%%%%%%benchmark%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{russakovsky2015imagenet,
title={Imagenet large scale visual recognition challenge},
author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
journal={International Journal of Computer Vision},
volume={115},
number={3},
pages={211--252},
year={2015},
publisher={Springer}
}


@inproceedings{jbig,
  title={Coded Representation of Picture and Audio Information Progressive Bi-level Image Compression},
  booktitle={ISO/IEC International Standard 11544:ITU-T Rec.T.82},
  year={1993}
}

@inproceedings{le2013building,
  title={Building high-level features using large scale unsupervised learning},
  author={Le, Quoc V},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
  pages={8595--8598},
  year={2013},
  organization={IEEE}
}

@inproceedings{coates2013deep,
  title={Deep learning with COTS HPC systems},
  author={Coates, Adam and Huval, Brody and Wang, Tao and Wu, David and Catanzaro, Bryan and Andrew, Ng},
  booktitle={Proceedings of The 30th International Conference on Machine Learning},
  pages={1337--1345},
  year={2013}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{sak2014long,
  title={Long short-term memory recurrent neural network architectures for large scale acoustic modeling.},
  author={Sak, Hasim and Senior, Andrew W and Beaufays, Fran{\c{c}}oise},
  booktitle={Interspeech},
  pages={338--342},
  year={2014}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}

@misc{krizhevsky2012cuda,
  title={cuda-convnet: High-performance c++/cuda implementation of convolutional neural networks},
  author={Krizhevsky, Alex},
  year={2012}
}

%%%%%%%%%%%%%%%%ACC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{chen2014diannao,
author = {Chen, Tianshi and Du, Zidong and Sun, Ninghui and Wang, Jia and Wu, Chengyong and Chen, Yunji and Temam, Olivier},
booktitle = {Proceedings of the 19th international conference on Architectural support for programming languages and operating systems (ASPLOS)},
file = {:home/zidong/work/Mendeley Library/Chen et al.{\_}2014{\_}DianNao a small-footprint high-throughput accelerator for ubiquitous machine-learning(2).pdf:pdf;:home/zidong/work/Mendeley Library/Chen et al.{\_}2014{\_}DianNao a small-footprint high-throughput accelerator for ubiquitous machine-learning.pdf:pdf},
isbn = {9781450323055},
mendeley-groups = {ML{\_}NN},
pages = {269--284},
title = {{DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning}},
url = {http://dl.acm.org/citation.cfm?id=2541967},
year = {2014}
}

@inproceedings{chen2014dadiannao,
  title={Dadiannao: A machine-learning supercomputer},
  author={Chen, Yunji and Luo, Tao and Liu, Shaoli and Zhang, Shijin and He, Liqiang and Wang, Jia and Li, Ling and Chen, Tianshi and Xu, Zhiwei and Sun, Ninghui and others},
  booktitle={Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={609--622},
  year={2014},
  organization={IEEE Computer Society}
}

@inproceedings{du2015shidiannao,
  title={ShiDianNao: Shifting vision processing closer to the sensor},
  author={Du, Zidong and Fasthuber, Robert and Chen, Tianshi and Ienne, Paolo and Li, Ling and Luo, Tao and Feng, Xiaobing and Chen, Yunji and Temam, Olivier},
  booktitle={ACM SIGARCH Computer Architecture News},
  volume={43},
  number={3},
  pages={92--104},
  year={2015},
  organization={ACM}
}



@inproceedings{liu2015pudiannao,
title={Pudiannao: A polyvalent machine learning accelerator},
author={Liu, Daofu and Chen, Tianshi and Liu, Shaoli and Zhou, Jinhong and Zhou, Shengyuan and Teman, Olivier and Feng, Xiaobing and Zhou, Xuehai and Chen, Yunji},
booktitle={ACM SIGARCH Computer Architecture News},
volume={43},
number={1},
pages={369--381},
year={2015},
organization={ACM}
}

@inproceedings{liu2016cambricon,
title={Cambricon: An instruction set architecture for neural networks},
author={Liu, Shaoli and Du, Zidong and Tao, Jinhua and Han, Dong and Luo, Tao and Xie, Yuan and Chen, Yunji and Chen, Tianshi},
booktitle={ACM SIGARCH Computer Architecture News},
volume={44},
number={3},
pages={393--405},
year={2016},
organization={IEEE Press}
}


@inproceedings{zhang2016cambricon,
  title={Cambricon-X: An accelerator for sparse neural networks},
  author={Zhang, Shijin and Du, Zidong and Zhang, Lei and Lan, Huiying and Liu, Shaoli and Li, Ling and Guo, Qi and Chen, Tianshi and Chen, Yunji},
  booktitle={Microarchitecture (MICRO), 2016 49th Annual IEEE/ACM International Symposium on},
  pages={1--12},
  year={2016},
  organization={IEEE}
}

@inproceedings{albericio2016cnvlutin,
  title={Cnvlutin: Ineffectual-neuron-free deep neural network computing},
  author={Albericio, Jorge and Judd, Patrick and Hetherington, Tayler and Aamodt, Tor and Jerger, Natalie Enright and Moshovos, Andreas},
  booktitle={Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual International Symposium on},
  pages={1--13},
  year={2016},
  organization={IEEE}
}

@inproceedings{han2016eie,
  title={EIE: efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  booktitle={Proceedings of the 43rd International Symposium on Computer Architecture},
  pages={243--254},
  year={2016},
  organization={IEEE Press}
}

@inproceedings{han2017ese,
  title={ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA},
  author={Han, Song and Kang, Junlong and Mao, Huizi and Hu, Yiming and Li, Xin and Li, Yubin and Xie, Dongliang and Luo, Hong and Yao, Song and Wang, Yu and others},
  booktitle={Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={75--84},
  year={2017},
  organization={ACM}
}

@article{chen2017eyeriss,
title={Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks},
author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S and Sze, Vivienne},
journal={IEEE Journal of Solid-State Circuits},
volume={52},
number={1},
pages={127--138},
year={2017},
publisher={IEEE}
}

@inproceedings{jouppi2017tpu,
title={In-datacenter performance analysis of a tensor processing unit},
author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
booktitle={Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages={1--12},
year={2017},
organization={ACM}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%COMPRESS%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1135--1143},
  year={2015}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{wang2016cnnpack,
  title={CNNpack: Packing Convolutional Neural Networks in the Frequency Domain},
  author={Wang, Yunhe and Xu, Chang and You, Shan and Tao, Dacheng and Xu, Chao},
  booktitle={Advances In Neural Information Processing Systems},
  pages={253--261},
  year={2016}
}

@inproceedings{venkataramani2014axnn,
  title={AxNN: energy-efficient neuromorphic systems using approximate computing},
  author={Venkataramani, Swagath and Ranjan, Ashish and Roy, Kaushik and Raghunathan, Anand},
  booktitle={Proceedings of the 2014 international symposium on Low power electronics and design},
  pages={27--32},
  year={2014},
  organization={ACM}
}


@inproceedings{courbariaux2015binaryconnect,
title={Binaryconnect: Training deep neural networks with binary weights during propagations},
author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
booktitle={Advances in neural information processing systems},
pages={3123--3131},
year={2015}
}

@article{hu2018hashing,
title={From hashing to CNNs: Training BinaryWeight networks via hashing},
author={Hu, Qinghao and Wang, Peisong and Cheng, Jian},
journal={arXiv preprint arXiv:1802.02733},
year={2018}
}

@article{li2016ternary,
title={Ternary Weight Networks.(2016)},
author={Li, F and Liu, B},
journal={arXiv preprint arXiv:1605.04711},
year={2016}
}

@article{zhu2016trained,
title={Trained ternary quantization},
author={Zhu, Chenzhuo and Han, Song and Mao, Huizi and Dally, William J},
journal={arXiv preprint arXiv:1612.01064},
year={2016}
}

@article{zhou2017incremental,
title={Incremental network quantization: Towards lossless cnns with low-precision weights},
author={Zhou, Aojun and Yao, Anbang and Guo, Yiwen and Xu, Lin and Chen, Yurong},
journal={arXiv preprint arXiv:1702.03044},
year={2017}
}


@article{courbariaux2016binarized,
title={Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1},
author={Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
journal={arXiv preprint arXiv:1602.02830},
year={2016}
}

@inproceedings{rastegari2016xnor,
  title={Xnor-net: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={European Conference on Computer Vision},
  pages={525--542},
  year={2016},
  organization={Springer}
}

@inproceedings{hubara2016binarized,
title={Binarized neural networks},
author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
booktitle={Advances in neural information processing systems},
pages={4107--4115},
year={2016}
}

@article{zhou2016dorefa,
title={Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients},
author={Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
journal={arXiv preprint arXiv:1606.06160},
year={2016}
}


@article{cai2017deep,
title={Deep learning with low precision by half-wave gaussian quantization},
author={Cai, Zhaowei and He, Xiaodong and Sun, Jian and Vasconcelos, Nuno},
journal={arXiv preprint arXiv:1702.00953},
year={2017}
}




@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT press Cambridge}
}


@inproceedings{pillai2001real,
  title={Real-time dynamic voltage scaling for low-power embedded operating systems},
  author={Pillai, Padmanabhan and Shin, Kang G},
  booktitle={ACM SIGOPS Operating Systems Review},
  volume={35},
  number={5},
  pages={89--102},
  year={2001},
  organization={ACM}
}

@article{holi1993finite,
  title={Finite precision error analysis of neural network hardware implementations},
  author={Holi, Jordan L and Hwang, J-N},
  journal={IEEE Transactions on Computers},
  volume={42},
  number={3},
  pages={281--290},
  year={1993},
  publisher={IEEE}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting.},
  author={Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014}
}

@book{pratt1989comparing,
  title={Comparing biases for minimal network construction with back-propagation},
  author={Pratt, Lorien Y},
  volume={1},
  year={1989},
  publisher={Morgan Kaufmann Pub}
}

@book{mackay2003information,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David JC},
  year={2003},
  publisher={Cambridge university press}
}

@article{huffman1952method,
  title={A method for the construction of minimum-redundancy codes},
  author={Huffman, David A},
  journal={Proceedings of the IRE},
  volume={40},
  number={9},
  pages={1098--1101},
  year={1952},
  publisher={IEEE}
}

@article{witten1987arithmetic,
  title={Arithmetic coding for data compression},
  author={Witten, Ian H and Neal, Radford M and Cleary, John G},
  journal={Communications of the ACM},
  volume={30},
  number={6},
  pages={520--540},
  year={1987},
  publisher={ACM}
}

@book{henneaux1992quantization,
  title={Quantization of gauge systems},
  author={Henneaux, Marc and Teitelboim, Claudio},
  year={1992},
  publisher={Princeton university press}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%NN%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{larochelle2007empirical,
  title={An empirical evaluation of deep architectures on problems with many factors of variation},
  author={Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={473--480},
  year={2007},
  organization={ACM}
}

@inproceedings{salakhutdinov2007learning,
  title={Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure.},
  author={Salakhutdinov, Ruslan and Hinton, Geoffrey E},
  booktitle={AISTATS},
  volume={11},
  year={2007}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}

@article{olshausen1996emergence,
  title={Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
  author={Olshausen, Bruno A and Field, David J},
  journal={Nature},
  volume={381},
  number={6583},
  pages={607},
  year={1996},
  publisher={Nature Publishing Group}
}

@inproceedings{boureau2008sparse,
  title={Sparse feature learning for deep belief networks},
  author={Boureau, Y-lan and Cun, Yann L and others},
  booktitle={Advances in neural information processing systems},
  pages={1185--1192},
  year={2008}
}

@inproceedings{lee2008sparse,
  title={Sparse deep belief net model for visual area V2},
  author={Lee, Honglak and Ekanadham, Chaitanya and Ng, Andrew Y},
  booktitle={Advances in neural information processing systems},
  pages={873--880},
  year={2008}
}

@article{lee2007efficient,
  title={Efficient sparse coding algorithms},
  author={Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y},
  journal={Advances in neural information processing systems},
  volume={19},
  pages={801},
  year={2007},
  publisher={MIT; 1998}
}

@article{amodei2015deep,
  title={Deep speech 2: End-to-end speech recognition in english and mandarin},
  author={Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and others},
  journal={arXiv preprint arXiv:1512.02595},
  year={2015}
}

@inproceedings{ze2013statistical,
  title={Statistical parametric speech synthesis using deep neural networks},
  author={Ze, Heiga and Senior, Andrew and Schuster, Mike},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
  pages={7962--7966},
  year={2013},
  organization={IEEE}
}

@article{conneau2016very,
  title={Very deep convolutional networks for natural language processing},
  author={Conneau, Alexis and Schwenk, Holger and Barrault, Lo{\"\i}c and Lecun, Yann},
  journal={arXiv preprint arXiv:1606.01781},
  year={2016}
}

@inproceedings{noh2015learning,
  title={Learning deconvolution network for semantic segmentation},
  author={Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1520--1528},
  year={2015}
}


@article{pearlmutter1989learning,
  title={Learning state space trajectories in recurrent neural networks},
  author={Pearlmutter, Barak A},
  journal={Neural Computation},
  volume={1},
  number={2},
  pages={263--269},
  year={1989},
  publisher={MIT Press}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Experiment%%%%%%%%%%%%%%%%%%%%
@inproceedings{muralimanohar2007optimizing,
  title={Optimizing NUCA organizations and wiring alternatives for large caches with CACTI 6.0},
  author={Muralimanohar, Naveen and Balasubramonian, Rajeev and Jouppi, Norm},
  booktitle={Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={3--14},
  year={2007},
  organization={IEEE Computer Society}
}

@inproceedings{jia2014caffe,
  title={Caffe: Convolutional architecture for fast feature embedding},
  author={Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  booktitle={Proceedings of the 22nd ACM international conference on Multimedia},
  pages={675--678},
  year={2014},
  organization={ACM}
}

@inproceedings{abadi2016tensorflow,
title={Tensorflow: a system for large-scale machine learning.},
author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
booktitle={OSDI},
volume={16},
pages={265--283},
year={2016}
}

@article{chen2015mxnet,
title={Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems},
author={Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
journal={arXiv preprint arXiv:1512.01274},
year={2015}
}



@article{duff2002overview,
  title={An overview of the sparse basic linear algebra subprograms: The new standard from the BLAS technical forum},
  author={Duff, Iain S and Heroux, Michael A and Pozo, Roldan},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={28},
  number={2},
  pages={239--267},
  year={2002},
  publisher={ACM}
}

@inproceedings{nvidia_cuda,
  title={The nvidia cuda sparse matrix library (cusoarse)},
  author={NVIDIA}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Related work%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{gong2014compressing,
  title={Compressing deep convolutional networks using vector quantization},
  author={Gong, Yunchao and Liu, Liu and Yang, Ming and Bourdev, Lubomir},
  journal={arXiv preprint arXiv:1412.6115},
  year={2014}
}

@inproceedings{chen2015compressing,
  title={Compressing Neural Networks with the Hashing Trick.},
  author={Chen, Wenlin and Wilson, James T and Tyree, Stephen and Weinberger, Kilian Q and Chen, Yixin},
  booktitle={ICML},
  pages={2285--2294},
  year={2015}
}

@inproceedings{denton2014exploiting,
  title={Exploiting linear structure within convolutional networks for efficient evaluation},
  author={Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1269--1277},
  year={2014}
}

@article{jaderberg2014speeding,
title={Speeding up convolutional neural networks with low rank expansions},
author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
journal={arXiv preprint arXiv:1405.3866},
year={2014}
}

@article{lebedev2014speeding,
title={Speeding-up convolutional neural networks using fine-tuned cp-decomposition},
author={Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
journal={arXiv preprint arXiv:1412.6553},
year={2014}
}

@inproceedings{he2014reshaping,
  title={Reshaping deep neural network for fast decoding by node-pruning},
  author={He, Tianxing and Fan, Yuchen and Qian, Yanmin and Tan, Tian and Yu, Kai},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
  pages={245--249},
  year={2014},
  organization={IEEE}
}

@inproceedings{lecun1989optimal,
  title={Optimal brain damage.},
  author={LeCun, Yann and Denker, John S and Solla, Sara A and Howard, Richard E and Jackel, Lawrence D},
  booktitle={NIPs},
  volume={2},
  pages={598--605},
  year={1989}
}

@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={Neural Networks, 1993., IEEE International Conference on},
  pages={293--299},
  year={1993},
  organization={IEEE}
}

@inproceedings{farabet2011neuflow,
  title={Neuflow: A runtime reconfigurable dataflow processor for vision},
  author={Farabet, Cl{\'e}ment and Martini, Berin and Corda, Benoit and Akselrod, Polina and Culurciello, Eugenio and LeCun, Yann},
  booktitle={Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer Society Conference on},
  pages={109--116},
  year={2011},
  organization={IEEE}
}

@inproceedings{chakradhar2010dynamically,
  title={A dynamically configurable coprocessor for convolutional neural networks},
  author={Chakradhar, Srimat and Sankaradas, Murugan and Jakkula, Venkata and Cadambi, Srihari},
  booktitle={ACM SIGARCH Computer Architecture News},
  volume={38},
  number={3},
  pages={247--257},
  year={2010},
  organization={ACM}
}

@article{srinivas2015data,
  title={Data-free parameter pruning for deep neural networks},
  author={Srinivas, Suraj and Babu, R Venkatesh},
  journal={arXiv preprint arXiv:1507.06149},
  year={2015}
}

@article{hu2016network,
  title={Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures},
  author={Hu, Hengyuan and Peng, Rui and Tai, Yu-Wing and Tang, Chi-Keung},
  journal={arXiv preprint arXiv:1607.03250},
  year={2016}
}

@article{mariet2015diversity,
  title={Diversity networks},
  author={Mariet, Zelda and Sra, Suvrit},
  journal={arXiv preprint arXiv:1511.05077},
  year={2015}
}

@article{angshuman2017scnn,
  title={SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks},
  author={Angshuman, Parashar and Minsoo, Rhu and Anurag, Mukkara and Antonio, Puglielli and Rangharajan, Venkatesan and Brucek, Khailany and Joe, Emer and Stephen,Keckler and William, J. Dally},
  journal={In 44th International Symposium on Computer Architecture},
  year={2017}
}

@article{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={arXiv preprint arXiv:1608.08710},
  year={2016}
}

@inproceedings{wen2016learning,
  title={Learning structured sparsity in deep neural networks},
  author={Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2074--2082},
  year={2016}
}

@inproceedings{lebedev2016fast,
  title={Fast convnets using group-wise brain damage},
  author={Lebedev, Vadim and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2554--2564},
  year={2016}
}

@article{mao2017exploring,
  title={Exploring the Regularity of Sparse Structure in Convolutional Neural Networks},
  author={Mao, Huizi and Han, Song and Pool, Jeff and Li, Wenshuo and Liu, Xingyu and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1705.08922},
  year={2017}
}

@inproceedings{yu2017scalpel,
  title={Scalpel: Customizing dnn pruning to the underlying hardware parallelism},
  author={Yu, Jiecao and Lukefahr, Andrew and Palframan, David and Dasika, Ganesh and Das, Reetuparna and Mahlke, Scott},
  booktitle={Proceedings of the 44th Annual International Symposium on Computer Architecture},
  pages={548--560},
  year={2017},
  organization={ACM}
}

@inproceedings{hill2017deftnn,
  title={DeftNN: addressing bottlenecks for DNN execution on GPUs via synapse vector elimination and near-compute data fission},
  author={Hill, Parker and Jain, Animesh and Hill, Mason and Zamirai, Babak and Hsu, Chang-Hong and Laurenzano, Michael A and Mahlke, Scott and Tang, Lingjia and Mars, Jason},
  booktitle={Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={786--799},
  year={2017},
  organization={ACM}
}

// normalization
@article{ioffe2015batch,
title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
author={Ioffe, Sergey and Szegedy, Christian},
journal={arXiv preprint arXiv:1502.03167},
year={2015}
}

@article{ba2016layer,
title={Layer normalization},
author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
journal={arXiv preprint arXiv:1607.06450},
year={2016}
}

@article{dmitry2016instance,
title={Instance Normalization: The Missing Ingredient for Fast Stylization},
author={Dmitry, Ulyanov and Andrea, Vedaldi and Victor, Lempitsky},
journal={arXiv preprint arXiv:1607.08022},
year={2016}
}


@article{wu2018group,
title={Group normalization},
author={Wu, Yuxin and He, Kaiming},
journal={arXiv preprint arXiv:1803.08494},
year={2018}
}



@inproceedings{koster2017flexpoint,
title={Flexpoint: An adaptive numerical format for efficient training of deep neural networks},
author={K{\"o}ster, Urs and Webb, Tristan and Wang, Xin and Nassar, Marcel and Bansal, Arjun K and Constable, William and Elibol, Oguz and Gray, Scott and Hall, Stewart and Hornof, Luke and others},
booktitle={Advances in Neural Information Processing Systems},
pages={1742--1752},
year={2017}
}

@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different 鈥渢hinned鈥?networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:home/zidong/work/Mendeley Library/Srivastava et al.{\_}2014{\_}Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
mendeley-groups = {NN/ML/Algorithm},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}


@inproceedings{gupta2015deep,
title={Deep learning with limited numerical precision},
author={Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
booktitle={International Conference on Machine Learning},
pages={1737--1746},
year={2015}
}

@article{dettmers20158,
title={8-bit approximations for parallelism in deep learning},
author={Dettmers, Tim},
journal={arXiv preprint arXiv:1511.04561},
year={2015}
}

@inproceedings{wang2017fixed,
title={Fixed-point factorized networks},
author={Wang, Peisong and Cheng, Jian},
booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},
pages={3966--3974},
year={2017},
organization={IEEE}
}











