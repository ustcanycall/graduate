\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{ustcthesis-numerical}
\HyPL@Entry{0<</P()>>}
\providecommand {\FN@pp@footnotehinttrue }{}
\providecommand {\FN@pp@footnote@aux }[2]{}
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\pgfsyspdfmark {pgfid1}{7545713}{47508770}
\pgfsyspdfmark {pgfid2}{7545713}{47508770}
\pgfsyspdfmark {pgfid3}{7545713}{47508770}
\pgfsyspdfmark {pgfid4}{7545713}{47508770}
\pgfsyspdfmark {pgfid5}{7545713}{47508770}
\pgfsyspdfmark {pgfid6}{7545713}{47508770}
\FN@pp@footnotehinttrue 
\HyPL@Entry{2<</P()>>}
\FN@pp@footnotehinttrue 
\pgfsyspdfmark {pgfid7}{7545713}{47508770}
\pgfsyspdfmark {pgfid8}{7545713}{47508770}
\pgfsyspdfmark {pgfid9}{7545713}{47508770}
\pgfsyspdfmark {pgfid10}{7545713}{47508770}
\pgfsyspdfmark {pgfid11}{7545713}{47508770}
\pgfsyspdfmark {pgfid12}{7545713}{47508770}
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\HyPL@Entry{6<</S/R>>}
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\citation{mcculloch1943logical}
\citation{hebb1963organizations,gerstner2002mathematical}
\HyPL@Entry{12<</S/D>>}
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {chapter}{\numberline {第1章\hspace  {0.3em}}绪论}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}神经网络算法发展历程}{1}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}第一个时期（1940年代到50年代）}{1}{subsection.1.1.1}}
\citation{hodgkin1952quantitative}
\citation{taylor1956electrical}
\citation{rosenblatt1958perceptron}
\citation{widrow1960adaptive}
\citation{fitzhugh1961impulses}
\citation{minsky1967computation}
\citation{minsky5paper}
\citation{cainiello1961outline}
\citation{nagumo1972response}
\citation{hopfield1982neural}
\citation{zurada1996generalized}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}第二个时期 (1960年代到70年代)}{2}{subsection.1.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}第三个时期（1980年代到90年代）}{2}{subsection.1.1.3}}
\citation{hindmarsh1984model}
\citation{ackley1985learning}
\citation{kirkpatrick1983optimization,vcerny1985thermodynamical}
\citation{rumelhart1986learning}
\citation{minsky5paper}
\citation{mackay1992practical,bishop1995neural}
\citation{williams1996gaussian}
\citation{vapnik1998statistical}
\citation{krizhevsky2012imagenet,simonyan2014very,ren2015faster}
\citation{hinton2012deep,amodei2015deep,ze2013statistical}
\citation{conneau2016very}
\citation{moyer2016google}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}第四个时期（2000年代至今）}{3}{subsection.1.1.4}}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\citation{deng2009imagenet}
\citation{szegedy2015going}
\citation{he2016deep}
\citation{girshick2014rich}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}神经网络在图像分类领域的应用}{4}{subsubsection.1.1.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}神经网络在目标检测领域的应用}{4}{subsubsection.1.1.4.2}}
\citation{girshick2015fast}
\citation{he2014spatial}
\citation{ren2015faster}
\citation{he2017mask}
\citation{redmon2016you}
\citation{gupta2015deep}
\citation{dettmers20158}
\citation{courbariaux2015binaryconnect}
\citation{rastegari2016xnor}
\citation{zhou2017incremental}
\citation{reed1993pruning,lecun1989optimal,miche2010op,han2015learning}
\citation{denton2014exploiting,lebedev2014speeding}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}神经网络的低能耗技术}{5}{subsubsection.1.1.4.3}}
\citation{chakradhar2010dynamically,vanhoucke2011improving}
\citation{farabet2009cnp,scherer2010accelerating,ciresan2011flexible,coates2013deep}
\citation{jia2014caffe}
\citation{abadi2016tensorflow}
\citation{chen2015mxnet}
\citation{vanhoucke2011improving}
\citation{dean2012large}
\citation{oh2004gpu}
\citation{scherer2010accelerating}
\citation{le2013building}
\citation{coates2013deep}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}神经网络计算平台发展}{6}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}通用处理器CPU和GPU}{6}{subsection.1.2.1}}
\citation{farabet2009cnp}
\citation{farabet2011neuflow}
\citation{gokhale2014240}
\citation{zhang2015optimizing}
\citation{suda2016throughput}
\citation{zhang2015optimizing}
\citation{qiu2016going}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}FPGA}{7}{subsection.1.2.2}}
\citation{rice2009scaling}
\citation{kim2009highly}
\citation{lee1987parallel}
\citation{stearns1988reconfigurable}
\citation{kamp1990programmable}
\citation{hecht1991advanced}
\citation{lee2006super}
\citation{chen2016diannao}
\citation{chen2014diannao}
\citation{chen2014diannao}
\citation{liu2015pudiannao}
\citation{du2015shidiannao}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}ASIC}{8}{subsection.1.2.3}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces DianNao Family详细参数\relax }}{8}{table.caption.4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:diannao_family}{{1.1}{8}{DianNao Family详细参数\relax }{table.caption.4}{}}
\citation{farabet2011neuflow}
\citation{chen2016eyeriss}
\citation{jouppi2017tpu}
\citation{zhang2016cambricon}
\citation{chen2017eyeriss,zhang2016cambricon,albericio2016cnvlutin,han2016eie,han2017ese,angshuman2017scnn}
\citation{chen2017eyeriss}
\citation{zhang2016cambricon}
\citation{albericio2016cnvlutin}
\citation{han2016eie}
\citation{han2017ese}
\citation{angshuman2017scnn}
\citation{han2015learning,han2015deep,wang2016cnnpack,zhou2017incremental}
\citation{chen2017eyeriss,zhang2016cambricon,albericio2016cnvlutin,han2016eie,han2017ese,angshuman2017scnn}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces 现有支持稀疏的神经网络加速器的比较\relax }}{9}{table.caption.5}}
\newlabel{tab:comp}{{1.2}{9}{现有支持稀疏的神经网络加速器的比较\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}主要研究内容及贡献}{9}{section.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces 本文主要研究内容和创新点\relax }}{10}{figure.caption.6}}
\newlabel{fig:idea}{{1.1}{10}{本文主要研究内容和创新点\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}论文的组织结构}{11}{section.1.4}}
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {chapter}{\numberline {第2章\hspace  {0.3em}}神经网络简介}{12}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}神经网络算法基础}{12}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}全连接层}{12}{subsection.2.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \relax \fontsize  {9\bp@ }{15\bp@ }\selectfont  \abovedisplayskip 9\bp@ plus2\bp@ minus5\bp@ \abovedisplayshortskip \z@ plus3\bp@ \belowdisplayshortskip 6\bp@ plus3\bp@ minus3\bp@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip 全连接层\relax }}{12}{figure.caption.7}}
\newlabel{fig:fc_layer}{{2.1}{12}{\footnotesize 全连接层\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}卷积层}{13}{subsection.2.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \relax \fontsize  {9\bp@ }{15\bp@ }\selectfont  \abovedisplayskip 9\bp@ plus2\bp@ minus5\bp@ \abovedisplayshortskip \z@ plus3\bp@ \belowdisplayshortskip 6\bp@ plus3\bp@ minus3\bp@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip 卷积层\relax }}{13}{figure.caption.8}}
\newlabel{fig:conv_layer}{{2.2}{13}{\footnotesize 卷积层\relax }{figure.caption.8}{}}
\citation{angshuman2017scnn}
\newlabel{list:convcode}{{2.1}{14}{六层卷积循环}{lstlisting.2.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2.1}六层卷积循环}{14}{lstlisting.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}池化层}{14}{subsection.2.1.3}}
\citation{krizhevsky2012imagenet}
\citation{ioffe2015batch}
\citation{ba2016layer}
\citation{dmitry2016instance}
\citation{wu2018group}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}归一化层}{15}{subsection.2.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \relax \fontsize  {9\bp@ }{15\bp@ }\selectfont  \abovedisplayskip 9\bp@ plus2\bp@ minus5\bp@ \abovedisplayshortskip \z@ plus3\bp@ \belowdisplayshortskip 6\bp@ plus3\bp@ minus3\bp@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip 归一化方法。每一个子图显示的是一个feature map，其中N为batch轴，C为channel轴，（H,W）为空间轴。蓝色的像素表示归一化的范围。\relax }}{15}{figure.caption.9}}
\newlabel{fig:norm}{{2.3}{15}{\footnotesize 归一化方法。每一个子图显示的是一个feature map，其中N为batch轴，C为channel轴，（H,W）为空间轴。蓝色的像素表示归一化的范围。\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}激活层}{16}{subsection.2.1.5}}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}LSTM}{17}{subsection.2.1.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \relax \fontsize  {9\bp@ }{15\bp@ }\selectfont  \abovedisplayskip 9\bp@ plus2\bp@ minus5\bp@ \abovedisplayshortskip \z@ plus3\bp@ \belowdisplayshortskip 6\bp@ plus3\bp@ minus3\bp@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip (a) LSTM (b) GRU\relax }}{17}{figure.caption.10}}
\newlabel{fig:RNN}{{2.4}{17}{\footnotesize (a) LSTM (b) GRU\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.7}GRU}{18}{subsection.2.1.7}}
\citation{le2013building,coates2013deep}
\citation{gupta2015deep}
\citation{goodfellow2016deep}
\citation{gupta2015deep}
\citation{koster2017flexpoint}
\citation{dettmers20158}
\citation{courbariaux2015binaryconnect}
\citation{hu2018hashing}
\citation{rastegari2016xnor}
\citation{russakovsky2015imagenet}
\citation{li2016ternary,zhu2016trained}
\citation{zhu2016trained}
\citation{zhou2017incremental}
\citation{wang2017fixed}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}神经网络低能耗的技术}{19}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}神经网络的低精度计算}{19}{subsection.2.2.1}}
\citation{hubara2016binarized}
\citation{rastegari2016xnor}
\citation{zhou2016dorefa}
\citation{cai2017deep}
\citation{han2015learning}
\citation{han2016eie}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}神经网络压缩模型}{20}{subsection.2.2.2}}
\citation{wang2016cnnpack}
\citation{he2014reshaping,srinivas2015data,hu2016network,mariet2015diversity}
\citation{srinivas2015data}
\citation{han2015learning}
\citation{hu2016network}
\citation{han2015learning}
\citation{denton2014exploiting,jaderberg2014speeding,lebedev2014speeding}
\citation{denton2014exploiting}
\citation{jaderberg2014speeding}
\citation{jaderberg2014speeding}
\citation{lebedev2014speeding}
\citation{chen2014diannao}
\citation{chen2014dadiannao}
\citation{liu2015pudiannao}
\citation{liu2016cambricon}
\citation{zhang2016cambricon}
\citation{albericio2016cnvlutin}
\citation{han2016eie}
\citation{han2017ese}
\citation{du2015shidiannao}
\citation{chen2017eyeriss}
\citation{jouppi2017tpu}
\citation{farabet2011neuflow}
\citation{angshuman2017scnn}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}神经网络加速器}{21}{section.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}现有神经网络加速器架构}{21}{subsection.2.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces \relax \fontsize  {9\bp@ }{15\bp@ }\selectfont  \abovedisplayskip 9\bp@ plus2\bp@ minus5\bp@ \abovedisplayshortskip \z@ plus3\bp@ \belowdisplayshortskip 6\bp@ plus3\bp@ minus3\bp@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip 现有的加速器架构.\relax }}{22}{figure.caption.12}}
\newlabel{fig:dataflow}{{2.5}{22}{\footnotesize 现有的加速器架构.\relax }{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces \relax \fontsize  {9\bp@ }{15\bp@ }\selectfont  \abovedisplayskip 9\bp@ plus2\bp@ minus5\bp@ \abovedisplayshortskip \z@ plus3\bp@ \belowdisplayshortskip 6\bp@ plus3\bp@ minus3\bp@ \def \leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep {\leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip 现有神经网络加速器的数据流形式\relax }}{22}{table.caption.11}}
\newlabel{tab:dataflow}{{2.1}{22}{\footnotesize 现有神经网络加速器的数据流形式\relax }{table.caption.11}{}}
\citation{chen2014diannao}
\citation{chen2014dadiannao}
\citation{liu2015pudiannao}
\citation{liu2016cambricon}
\citation{zhang2016cambricon}
\citation{albericio2016cnvlutin}
\citation{han2016eie}
\citation{han2017ese}
\citation{du2015shidiannao}
\citation{chen2017eyeriss}
\citation{jouppi2017tpu}
\citation{farabet2011neuflow}
\citation{angshuman2017scnn}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \relax \fontsize  {9\bp@ }{15\bp@ }\selectfont  \abovedisplayskip 9\bp@ plus2\bp@ minus5\bp@ \abovedisplayshortskip \z@ plus3\bp@ \belowdisplayshortskip 6\bp@ plus3\bp@ minus3\bp@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip 脉动阵列完成矩阵乘法操作\relax }}{23}{figure.caption.13}}
\newlabel{fig:systolic}{{2.6}{23}{\footnotesize 脉动阵列完成矩阵乘法操作\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}基于向量算子的神经网络处理器}{23}{subsection.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces \relax \fontsize  {9\bp@ }{15\bp@ }\selectfont  \abovedisplayskip 9\bp@ plus2\bp@ minus5\bp@ \abovedisplayshortskip \z@ plus3\bp@ \belowdisplayshortskip 6\bp@ plus3\bp@ minus3\bp@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip （a）DianNao加速器结构。（b）DaDianNao一个tile的结构。 （c）DaDianNao一个node的结构\relax }}{24}{figure.caption.14}}
\newlabel{fig:diannao}{{2.7}{24}{\footnotesize （a）DianNao加速器结构。（b）DaDianNao一个tile的结构。 （c）DaDianNao一个node的结构\relax }{figure.caption.14}{}}
\citation{chen2017eyeriss,zhang2016cambricon,albericio2016cnvlutin,han2016eie,han2017ese,angshuman2017scnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}基于乘加算子空间数据流的神经网络处理器}{25}{subsection.2.3.3}}
\citation{chen2017eyeriss}
\citation{zhang2016cambricon}
\citation{albericio2016cnvlutin}
\citation{han2017ese}
\citation{han2016eie}
\citation{angshuman2017scnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}稀疏神经网络处理器}{26}{subsection.2.3.4}}
\FN@pp@footnotehinttrue 
\citation{srivastava2014dropout}
\citation{han2015learning}
\citation{holi1993finite}
\citation{rastegari2016xnor}
\citation{venkataramani2014axnn}
\citation{pillai2001real}
\citation{olshausen1996emergence}
\citation{boureau2008sparse,lee2008sparse}
\citation{lee2007efficient}
\citation{han2015learning}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\@writefile{toc}{\contentsline {chapter}{\numberline {第3章\hspace  {0.3em}}一种新的神经网络压缩方法}{28}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}背景}{28}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}神经网络中的稀疏特性}{29}{subsection.3.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  (a) 稠密神经网络 (b) 静态权值稀疏 (c) 静态神经元稀疏 (d) 动态神经元稀疏\relax }}{29}{figure.caption.15}}
\newlabel{fig:sparsity}{{3.1}{29}{(a) 稠密神经网络 (b) 静态权值稀疏 (c) 静态神经元稀疏 (d) 动态神经元稀疏\relax }{figure.caption.15}{}}
\citation{henneaux1992quantization}
\citation{mackay2003information}
\citation{han2015deep}
\citation{wang2016cnnpack}
\citation{}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}权值编码}{30}{subsection.3.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces 权值编码过程\relax }}{30}{figure.caption.16}}
\newlabel{fig:weight_encoding}{{3.2}{30}{权值编码过程\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}量化}{30}{subsubsection.3.1.2.1}}
\citation{huffman1952method}
\citation{witten1987arithmetic}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}熵编码}{31}{subsubsection.3.1.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces 哈弗曼树\relax }}{31}{figure.caption.17}}
\newlabel{fig:huffman}{{3.3}{31}{哈弗曼树\relax }{figure.caption.17}{}}
\citation{zhang2016cambricon}
\citation{zhang2016cambricon,albericio2016cnvlutin,han2016eie,han2017ese,angshuman2017scnn}
\citation{zhang2016cambricon}
\citation{albericio2016cnvlutin}
\citation{han2016eie}
\citation{angshuman2017scnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}不规则性}{32}{subsection.3.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}局部收敛 (local convergence)}{33}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces 全连接层中的局部收敛现象（白点表示大权值，绝对值大于其余$90\%$的权值）\relax }}{33}{figure.caption.18}}
\newlabel{fig:local_convergence}{{3.4}{33}{全连接层中的局部收敛现象（白点表示大权值，绝对值大于其余$90\%$的权值）\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces 较大权值的累计分布\relax }}{34}{figure.caption.19}}
\newlabel{fig:cdf}{{3.5}{34}{较大权值的累计分布\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces 新的压缩神经网络步骤\relax }}{34}{figure.caption.20}}
\newlabel{fig:compression_flow}{{3.6}{34}{新的压缩神经网络步骤\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}压缩神经网络}{34}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}粗粒度剪枝}{34}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}剪枝策略}{34}{subsubsection.3.3.1.1}}
\citation{han2015learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces 全连接层上的粗粒度剪枝\relax }}{35}{figure.caption.21}}
\newlabel{fig:fc_pruning}{{3.7}{35}{全连接层上的粗粒度剪枝\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces 卷积层上的粗粒度剪枝\relax }}{35}{figure.caption.22}}
\newlabel{fig:conv_pruning}{{3.8}{35}{卷积层上的粗粒度剪枝\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}剪枝块大小}{35}{subsubsection.3.3.1.2}}
\citation{han2015deep}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}最大值剪枝 vs. 平均值剪枝}{36}{subsubsection.3.3.1.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  不同剪枝块大小情况下AlexNet网络的稀疏度和压缩比（C: 卷积层；F： 全连接层; S: 稀疏度; $r_c$ 压缩比）\relax }}{36}{table.caption.23}}
\newlabel{tab:blocksize}{{3.1}{36}{不同剪枝块大小情况下AlexNet网络的稀疏度和压缩比（C: 卷积层；F： 全连接层; S: 稀疏度; $r_c$ 压缩比）\relax }{table.caption.23}{}}
\citation{jbig}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Cifar10快速模型上的最大值剪枝和平均值剪枝\relax }}{37}{figure.caption.24}}
\newlabel{fig:max_or_avg_pruning}{{3.9}{37}{Cifar10快速模型上的最大值剪枝和平均值剪枝\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}不规则性的评估方法}{37}{subsubsection.3.3.1.4}}
\citation{han2015deep}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5}神经元稀疏}{38}{subsubsection.3.3.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}局部量化}{38}{subsection.3.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces \relax \fontsize  {9\bp@ }{15\bp@ }\selectfont  \abovedisplayskip 9\bp@ plus2\bp@ minus5\bp@ \abovedisplayshortskip \z@ plus3\bp@ \belowdisplayshortskip 6\bp@ plus3\bp@ minus3\bp@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip 神经网络中的稀疏性 （C: 卷积层; F：全连接层; SSS： 静态权值稀疏; SNS: 静态神经元稀疏; DNS： 动态神经元稀疏）.\relax }}{38}{table.caption.25}}
\newlabel{tab:sparsities}{{3.2}{38}{\footnotesize 神经网络中的稀疏性 （C: 卷积层; F：全连接层; SSS： 静态权值稀疏; SNS: 静态神经元稀疏; DNS： 动态神经元稀疏）.\relax }{table.caption.25}{}}
\citation{huffman1952method}
\citation{witten1987arithmetic}
\citation{lecun1998gradient}
\citation{Srivastava2014}
\citation{krizhevsky2012cuda}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\citation{he2016deep}
\citation{sak2014long}
\citation{han2015deep}
\citation{wang2016cnnpack}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces 局部量化\relax }}{39}{figure.caption.26}}
\newlabel{fig:local_quantization}{{3.10}{39}{局部量化\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}熵编码}{39}{subsection.3.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}压缩实验结果}{39}{subsection.3.3.4}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces 压缩后神经网络的稀疏度，压缩比和不规则性减少量 ($W_p$: 粗粒度剪枝后权值规模 ; $W_q$: 粗粒度剪枝，局部量化后权值规模; $W_c$: 粗粒度剪枝，局部量化，熵编码后权值规模; L: \emph  {LSTM}层 ; C: 卷积层; F: 全连接层; W: 权值; I：权值索引；$r_p$: 粗粒度剪枝后压缩比; $r_q$: 粗粒度剪枝，局部量化后压缩比; $r_c$: 粗粒度剪枝，局部量化，熵编码后压缩比; $R(Irr)$: 不规则性减少量)。\relax }}{40}{table.caption.27}}
\newlabel{tab:compression}{{3.3}{40}{压缩后神经网络的稀疏度，压缩比和不规则性减少量 ($W_p$: 粗粒度剪枝后权值规模 ; $W_q$: 粗粒度剪枝，局部量化后权值规模; $W_c$: 粗粒度剪枝，局部量化，熵编码后权值规模; L: \emph {LSTM}层 ; C: 卷积层; F: 全连接层; W: 权值; I：权值索引；$r_p$: 粗粒度剪枝后压缩比; $r_q$: 粗粒度剪枝，局部量化后压缩比; $r_c$: 粗粒度剪枝，局部量化，熵编码后压缩比; $R(Irr)$: 不规则性减少量)。\relax }{table.caption.27}{}}
\citation{han2015deep}
\citation{wang2016cnnpack}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces \relax \fontsize  {9\bp@ }{15\bp@ }\selectfont  \abovedisplayskip 9\bp@ plus2\bp@ minus5\bp@ \abovedisplayshortskip \z@ plus3\bp@ \belowdisplayshortskip 6\bp@ plus3\bp@ minus3\bp@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip CNNPack, Deep Compression 与我们的压缩方法的对比 (S\%: 稀疏度; $r_c$: 压缩比).\relax }}{41}{table.caption.28}}
\newlabel{tab:deepratio}{{3.4}{41}{\footnotesize CNNPack, Deep Compression 与我们的压缩方法的对比 (S\%: 稀疏度; $r_c$: 压缩比).\relax }{table.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces AlexNet压缩特征 (W: 权值; S\%: 稀疏度 I: 权值索引).\relax }}{41}{table.caption.29}}
\newlabel{tab:AlexNet}{{3.5}{41}{AlexNet压缩特征 (W: 权值; S\%: 稀疏度 I: 权值索引).\relax }{table.caption.29}{}}
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {chapter}{\numberline {第4章\hspace  {0.3em}}粗粒度稀疏神经网络加速器}{43}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}设计原则}{43}{section.4.1}}
\newlabel{sec:principle}{{4.1}{43}{设计原则}{section.4.1}{}}
\citation{han2017ese}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces 粗粒度稀疏的全连接层\relax }}{44}{figure.caption.30}}
\newlabel{fig:connection}{{4.1}{44}{粗粒度稀疏的全连接层\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}加速器架构}{45}{section.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces 加速器整体架构\relax }}{45}{figure.caption.31}}
\newlabel{fig:acc}{{4.2}{45}{加速器整体架构\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}稀疏处理单元}{46}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Indexing}{46}{subsubsection.4.2.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}NSM}{47}{subsubsection.4.2.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces NSM结构\relax }}{47}{figure.caption.32}}
\newlabel{fig:NSM}{{4.3}{47}{NSM结构\relax }{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}NFU}{48}{subsubsection.4.2.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces PE结构\relax }}{48}{figure.caption.33}}
\newlabel{fig:PE}{{4.4}{48}{PE结构\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces (a) SSM结构 (b) PEFU结构\relax }}{49}{figure.caption.34}}
\newlabel{fig:PEFU}{{4.5}{49}{(a) SSM结构 (b) PEFU结构\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}Encoder}{49}{subsubsection.4.2.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Encoder架构\relax }}{49}{figure.caption.35}}
\newlabel{fig:encoder}{{4.6}{49}{Encoder架构\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}存储模块}{50}{subsection.4.2.2}}
\newlabel{subsec:storage}{{4.2.2}{50}{存储模块}{subsection.4.2.2}{}}
\citation{chen2014dadiannao,han2016eie}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}控制}{51}{subsection.4.2.3}}
\newlabel{subsec:control}{{4.2.3}{51}{控制}{subsection.4.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces CP的有限状态机\relax }}{51}{figure.caption.36}}
\newlabel{fig:FSM}{{4.7}{51}{CP的有限状态机\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}片上互联}{52}{subsection.4.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces H树互联结构\relax }}{52}{figure.caption.37}}
\newlabel{fig:Htree}{{4.8}{52}{H树互联结构\relax }{figure.caption.37}{}}
\citation{jia2014caffe}
\citation{abadi2016tensorflow}
\citation{chen2015mxnet}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}编程模型}{53}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}基于库的编程模型}{53}{subsection.4.3.1}}
\newlabel{list:conv}{{4.1}{53}{卷积层库函数接口}{lstlisting.4.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.1}卷积层库函数接口}{53}{lstlisting.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces 编程框架\relax }}{54}{figure.caption.38}}
\newlabel{fig:framework}{{4.9}{54}{编程框架\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}编译器}{54}{subsection.4.3.2}}
\citation{chen2017eyeriss}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces 源文件编译成为加速器可执行代码的过程\relax }}{55}{figure.caption.39}}
\newlabel{fig:compiler}{{4.10}{55}{源文件编译成为加速器可执行代码的过程\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}loop tiling}{55}{subsection.4.3.3}}
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {chapter}{\numberline {第5章\hspace  {0.3em}}性能验证和实验结果}{58}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}新型加速器性能验证}{58}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}背景}{58}{subsection.5.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}加速器专用性能模拟器}{59}{subsection.5.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}加速器的高层次抽象}{59}{subsubsection.5.1.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces 加速器的抽象结构\relax }}{60}{figure.caption.40}}
\newlabel{fig:simulator}{{5.1}{60}{加速器的抽象结构\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}事件分类}{61}{subsubsection.5.1.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}事件执行时间}{62}{subsubsection.5.1.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}事件的依赖关系}{63}{subsubsection.5.1.2.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5}模拟过程}{63}{subsubsection.5.1.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces 性能模拟器模拟神经网络在加速器上的执行过程\relax }}{63}{figure.caption.41}}
\newlabel{fig:simulation}{{5.2}{63}{性能模拟器模拟神经网络在加速器上的执行过程\relax }{figure.caption.41}{}}
\citation{muralimanohar2007optimizing}
\citation{jia2014caffe}
\citation{duff2002overview}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}实验方法}{65}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Baseline}{65}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}CPU}{65}{subsubsection.5.2.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}GPU}{65}{subsubsection.5.2.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}硬件加速器}{65}{subsubsection.5.2.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Benchmark}{66}{subsection.5.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}实验结果}{66}{section.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}硬件属性}{66}{subsection.5.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}性能}{66}{subsection.5.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces 加速器详细属性\relax }}{67}{table.caption.42}}
\newlabel{tab:hardware}{{5.1}{67}{加速器详细属性\relax }{table.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces 新型加速器与CPU，GPU，DianNao，Cambricon-X的性能对比\relax }}{67}{figure.caption.43}}
\newlabel{fig:total_performance}{{5.3}{67}{新型加速器与CPU，GPU，DianNao，Cambricon-X的性能对比\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces 新型加速器与CPU，GPU，DianNao，Cambricon-X在卷积层上的性能对比\relax }}{68}{figure.caption.44}}
\newlabel{fig:conv_performance}{{5.4}{68}{新型加速器与CPU，GPU，DianNao，Cambricon-X在卷积层上的性能对比\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces 新型加速器与CPU，GPU，DianNao，Cambricon-X在全连接层上的性能对比\relax }}{68}{figure.caption.45}}
\newlabel{fig:fc_performance}{{5.5}{68}{新型加速器与CPU，GPU，DianNao，Cambricon-X在全连接层上的性能对比\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}能耗}{69}{subsection.5.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces 新型加速器与GPU，DianNao，Cambricon-X在能耗的对比\relax }}{69}{figure.caption.46}}
\newlabel{fig:energy}{{5.6}{69}{新型加速器与GPU，DianNao，Cambricon-X在能耗的对比\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces 加速器在benchmark上的能耗分布（包括片外访存能耗）\relax }}{70}{figure.caption.47}}
\newlabel{fig:energy_breakdown}{{5.7}{70}{加速器在benchmark上的能耗分布（包括片外访存能耗）\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces 加速器在benchmark上的能耗分布（不包括片外访存能耗）\relax }}{70}{figure.caption.48}}
\newlabel{fig:energy_breakdown2}{{5.8}{70}{加速器在benchmark上的能耗分布（不包括片外访存能耗）\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}讨论}{71}{subsection.5.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}熵编码和熵解码模块}{71}{subsubsection.5.3.4.1}}
\newlabel{subsubsec:encoding_hw}{{1}{71}{熵编码和熵解码模块}{subsubsection.5.3.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}稀疏度与性能}{71}{subsubsection.5.3.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces 神经网络稀疏度对加速器性能的影响\relax }}{71}{figure.caption.49}}
\newlabel{fig:sensitivity}{{5.9}{71}{神经网络稀疏度对加速器性能的影响\relax }{figure.caption.49}{}}
\citation{yu2017scalpel}
\citation{hill2017deftnn}
\citation{wen2016learning,lebedev2016fast}
\citation{li2016pruning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}减少的不规则度}{72}{subsubsection.5.3.4.3}}
\citation{mao2017exploring}
\citation{han2016eie}
\citation{zhang2016cambricon}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}类似粗粒度稀疏的方法}{73}{subsubsection.5.3.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5}其他稀疏神经网络加速器}{73}{subsubsection.5.3.4.5}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces \relax \fontsize  {9\bp@ }{15\bp@ }\selectfont  \abovedisplayskip 9\bp@ plus2\bp@ minus5\bp@ \abovedisplayshortskip \z@ plus3\bp@ \belowdisplayshortskip 6\bp@ plus3\bp@ minus3\bp@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\bp@ plus2\bp@ minus2\bp@ \parsep 3\bp@ plus2\bp@ minus\bp@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip 与EIE的性能比较 \emph  {(microsecond)}.\relax }}{73}{table.caption.50}}
\newlabel{tab:EIE}{{5.2}{73}{\footnotesize 与EIE的性能比较 \emph {(microsecond)}.\relax }{table.caption.50}{}}
\citation{angshuman2017scnn}
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {chapter}{\numberline {第6章\hspace  {0.3em}}总结和展望}{75}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}本文工作总结}{75}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}未来研究展望}{76}{section.6.2}}
\bibdata{bib/ustc}
\FN@pp@footnotehinttrue 
\bibcite{mcculloch1943logical}{{1}{1943}{{McCulloch\ et~al.}}{{McCulloch and Pitts}}}
\bibcite{hebb1963organizations}{{2}{1963}{{Hebb}}{{}}}
\bibcite{gerstner2002mathematical}{{3}{2002}{{Gerstner\ et~al.}}{{Gerstner and Kistler}}}
\bibcite{hodgkin1952quantitative}{{4}{1952}{{Hodgkin\ et~al.}}{{Hodgkin and Huxley}}}
\bibcite{taylor1956electrical}{{5}{1956}{{Taylor}}{{}}}
\bibcite{rosenblatt1958perceptron}{{6}{1958}{{Rosenblatt}}{{}}}
\bibcite{widrow1960adaptive}{{7}{1960}{{Widrow\ et~al.}}{{Widrow and Hoff}}}
\bibcite{fitzhugh1961impulses}{{8}{1961}{{FitzHugh}}{{}}}
\bibcite{minsky1967computation}{{9}{1967}{{Minsky}}{{}}}
\bibcite{minsky5paper}{{10}{5}{{Minsky}}{{}}}
\bibcite{cainiello1961outline}{{11}{1961}{{Cainiello}}{{}}}
\bibcite{nagumo1972response}{{12}{1972}{{Nagumo\ et~al.}}{{Nagumo and Sato}}}
\bibcite{hopfield1982neural}{{13}{1982}{{Hopfield}}{{}}}
\bibcite{zurada1996generalized}{{14}{1996}{{Zurada\ et~al.}}{{Zurada, Cloete, and Van~der Poel}}}
\bibcite{hindmarsh1984model}{{15}{1984}{{Hindmarsh\ et~al.}}{{Hindmarsh and Rose}}}
\bibcite{ackley1985learning}{{16}{1985}{{Ackley\ et~al.}}{{Ackley, Hinton, and Sejnowski}}}
\bibcite{kirkpatrick1983optimization}{{17}{1983}{{Kirkpatrick\ et~al.}}{{Kirkpatrick, Gelatt, and Vecchi}}}
\@writefile{toc}{\contentsline {chapter}{参考文献}{78}{chapter*.51}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{vcerny1985thermodynamical}{{18}{1985}{{{\v {C}}ern{\`y}}}{{}}}
\bibcite{rumelhart1986learning}{{19}{1986}{{Rumelhart\ et~al.}}{{Rumelhart, Hinton, and Williams}}}
\bibcite{mackay1992practical}{{20}{1992}{{MacKay}}{{}}}
\bibcite{bishop1995neural}{{21}{1995}{{Bishop\ et~al.}}{{Bishop, Bishop, et~al.}}}
\bibcite{williams1996gaussian}{{22}{1996}{{Williams\ et~al.}}{{Williams and Rasmussen}}}
\bibcite{vapnik1998statistical}{{23}{1998}{{Vapnik}}{{}}}
\bibcite{krizhevsky2012imagenet}{{24}{2012}{{Krizhevsky\ et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{simonyan2014very}{{25}{2014}{{Simonyan\ et~al.}}{{Simonyan and Zisserman}}}
\bibcite{ren2015faster}{{26}{2015}{{Ren\ et~al.}}{{Ren, He, Girshick, and Sun}}}
\bibcite{hinton2012deep}{{27}{2012}{{Hinton\ et~al.}}{{Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior, Vanhoucke, Nguyen, Sainath, et~al.}}}
\bibcite{amodei2015deep}{{28}{2015}{{Amodei\ et~al.}}{{Amodei, Anubhai, Battenberg, Case, Casper, Catanzaro, Chen, Chrzanowski, Coates, Diamos, et~al.}}}
\bibcite{ze2013statistical}{{29}{2013}{{Ze\ et~al.}}{{Ze, Senior, and Schuster}}}
\bibcite{conneau2016very}{{30}{2016}{{Conneau\ et~al.}}{{Conneau, Schwenk, Barrault, and Lecun}}}
\bibcite{moyer2016google}{{31}{2016}{{Moyer}}{{}}}
\bibcite{deng2009imagenet}{{32}{2009}{{Deng\ et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{szegedy2015going}{{33}{2015}{{Szegedy\ et~al.}}{{Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, and Rabinovich}}}
\bibcite{he2016deep}{{34}{2016}{{He\ et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{girshick2014rich}{{35}{2014}{{Girshick\ et~al.}}{{Girshick, Donahue, Darrell, and Malik}}}
\bibcite{girshick2015fast}{{36}{2015}{{Girshick}}{{}}}
\bibcite{he2014spatial}{{37}{2014{}}{{He\ et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{he2017mask}{{38}{2017}{{He\ et~al.}}{{He, Gkioxari, Doll{\'a}r, and Girshick}}}
\bibcite{redmon2016you}{{39}{2016}{{Redmon\ et~al.}}{{Redmon, Divvala, Girshick, and Farhadi}}}
\bibcite{gupta2015deep}{{40}{2015}{{Gupta\ et~al.}}{{Gupta, Agrawal, Gopalakrishnan, and Narayanan}}}
\bibcite{dettmers20158}{{41}{2015}{{Dettmers}}{{}}}
\bibcite{courbariaux2015binaryconnect}{{42}{2015}{{Courbariaux\ et~al.}}{{Courbariaux, Bengio, and David}}}
\bibcite{rastegari2016xnor}{{43}{2016}{{Rastegari\ et~al.}}{{Rastegari, Ordonez, Redmon, and Farhadi}}}
\bibcite{zhou2017incremental}{{44}{2017}{{Zhou\ et~al.}}{{Zhou, Yao, Guo, Xu, and Chen}}}
\bibcite{reed1993pruning}{{45}{1993}{{Reed}}{{}}}
\bibcite{lecun1989optimal}{{46}{1989}{{LeCun\ et~al.}}{{LeCun, Denker, Solla, Howard, and Jackel}}}
\bibcite{miche2010op}{{47}{2010}{{Miche\ et~al.}}{{Miche, Sorjamaa, Bas, Simula, Jutten, and Lendasse}}}
\bibcite{han2015learning}{{48}{2015{}}{{Han\ et~al.}}{{Han, Pool, Tran, and Dally}}}
\bibcite{denton2014exploiting}{{49}{2014}{{Denton\ et~al.}}{{Denton, Zaremba, Bruna, LeCun, and Fergus}}}
\bibcite{lebedev2014speeding}{{50}{2014}{{Lebedev\ et~al.}}{{Lebedev, Ganin, Rakhuba, Oseledets, and Lempitsky}}}
\bibcite{chakradhar2010dynamically}{{51}{2010}{{Chakradhar\ et~al.}}{{Chakradhar, Sankaradas, Jakkula, and Cadambi}}}
\bibcite{vanhoucke2011improving}{{52}{2011}{{Vanhoucke\ et~al.}}{{Vanhoucke, Senior, and Mao}}}
\bibcite{farabet2009cnp}{{53}{2009}{{Farabet\ et~al.}}{{Farabet, Poulet, Han, and LeCun}}}
\bibcite{scherer2010accelerating}{{54}{2010}{{Scherer\ et~al.}}{{Scherer, Schulz, and Behnke}}}
\bibcite{ciresan2011flexible}{{55}{2011}{{Ciresan\ et~al.}}{{Ciresan, Meier, Masci, Maria~Gambardella, and Schmidhuber}}}
\bibcite{coates2013deep}{{56}{2013}{{Coates\ et~al.}}{{Coates, Huval, Wang, Wu, Catanzaro, and Andrew}}}
\bibcite{jia2014caffe}{{57}{2014}{{Jia\ et~al.}}{{Jia, Shelhamer, Donahue, Karayev, Long, Girshick, Guadarrama, and Darrell}}}
\bibcite{abadi2016tensorflow}{{58}{2016}{{Abadi\ et~al.}}{{Abadi, Barham, Chen, Chen, Davis, Dean, Devin, Ghemawat, Irving, Isard, et~al.}}}
\bibcite{chen2015mxnet}{{59}{2015}{{Chen\ et~al.}}{{Chen, Li, Li, Lin, Wang, Wang, Xiao, Xu, Zhang, and Zhang}}}
\bibcite{dean2012large}{{60}{2012}{{Dean\ et~al.}}{{Dean, Corrado, Monga, Chen, Devin, Mao, Senior, Tucker, Yang, Le, et~al.}}}
\bibcite{oh2004gpu}{{61}{2004}{{Oh\ et~al.}}{{Oh and Jung}}}
\bibcite{le2013building}{{62}{2013}{{Le}}{{}}}
\bibcite{farabet2011neuflow}{{63}{2011}{{Farabet\ et~al.}}{{Farabet, Martini, Corda, Akselrod, Culurciello, and LeCun}}}
\bibcite{gokhale2014240}{{64}{2014}{{Gokhale\ et~al.}}{{Gokhale, Jin, Dundar, Martini, and Culurciello}}}
\bibcite{zhang2015optimizing}{{65}{2015}{{Zhang\ et~al.}}{{Zhang, Li, Sun, Guan, Xiao, and Cong}}}
\bibcite{suda2016throughput}{{66}{2016}{{Suda\ et~al.}}{{Suda, Chandra, Dasika, Mohanty, Ma, Vrudhula, Seo, and Cao}}}
\bibcite{qiu2016going}{{67}{2016}{{Qiu\ et~al.}}{{Qiu, Wang, Yao, Guo, Li, Zhou, Yu, Tang, Xu, Song, et~al.}}}
\bibcite{rice2009scaling}{{68}{2009}{{Rice\ et~al.}}{{Rice, Taha, and Vutsinas}}}
\bibcite{kim2009highly}{{69}{2009}{{Kim\ et~al.}}{{Kim, McAfee, McMahon, and Olukotun}}}
\bibcite{lee1987parallel}{{70}{1987}{{Lee\ et~al.}}{{Lee and Aggarwal}}}
\bibcite{stearns1988reconfigurable}{{71}{1988}{{Stearns\ et~al.}}{{Stearns, Luthi, Ruetz, and Ang}}}
\bibcite{kamp1990programmable}{{72}{1990}{{Kamp\ et~al.}}{{Kamp, Kunemund, Soldner, and Hofer}}}
\bibcite{hecht1991advanced}{{73}{1991}{{Hecht\ et~al.}}{{Hecht and Ronner}}}
\bibcite{lee2006super}{{74}{2006}{{Lee\ et~al.}}{{Lee and Song}}}
\bibcite{chen2016diannao}{{75}{2016{}}{{Chen\ et~al.}}{{Chen, Chen, Xu, Sun, and Temam}}}
\bibcite{chen2014diannao}{{76}{2014{}}{{Chen\ et~al.}}{{Chen, Du, Sun, Wang, Wu, Chen, and Temam}}}
\bibcite{liu2015pudiannao}{{77}{2015}{{Liu\ et~al.}}{{Liu, Chen, Liu, Zhou, Zhou, Teman, Feng, Zhou, and Chen}}}
\bibcite{du2015shidiannao}{{78}{2015}{{Du\ et~al.}}{{Du, Fasthuber, Chen, Ienne, Li, Luo, Feng, Chen, and Temam}}}
\bibcite{chen2016eyeriss}{{79}{2016{}}{{Chen\ et~al.}}{{Chen, Emer, and Sze}}}
\bibcite{jouppi2017tpu}{{80}{2017}{{Jouppi\ et~al.}}{{Jouppi, Young, Patil, Patterson, Agrawal, Bajwa, Bates, Bhatia, Boden, Borchers, et~al.}}}
\bibcite{zhang2016cambricon}{{81}{2016}{{Zhang\ et~al.}}{{Zhang, Du, Zhang, Lan, Liu, Li, Guo, Chen, and Chen}}}
\bibcite{chen2017eyeriss}{{82}{2017}{{Chen\ et~al.}}{{Chen, Krishna, Emer, and Sze}}}
\bibcite{albericio2016cnvlutin}{{83}{2016}{{Albericio\ et~al.}}{{Albericio, Judd, Hetherington, Aamodt, Jerger, and Moshovos}}}
\bibcite{han2016eie}{{84}{2016}{{Han\ et~al.}}{{Han, Liu, Mao, Pu, Pedram, Horowitz, and Dally}}}
\bibcite{han2017ese}{{85}{2017}{{Han\ et~al.}}{{Han, Kang, Mao, Hu, Li, Li, Xie, Luo, Yao, Wang, et~al.}}}
\bibcite{angshuman2017scnn}{{86}{2017}{{Angshuman\ et~al.}}{{Angshuman, Minsoo, Anurag, Antonio, Rangharajan, Brucek, Joe, Stephen, and William}}}
\bibcite{han2015deep}{{87}{2015{}}{{Han\ et~al.}}{{Han, Mao, and Dally}}}
\bibcite{wang2016cnnpack}{{88}{2016}{{Wang\ et~al.}}{{Wang, Xu, You, Tao, and Xu}}}
\bibcite{ioffe2015batch}{{89}{2015}{{Ioffe\ et~al.}}{{Ioffe and Szegedy}}}
\bibcite{ba2016layer}{{90}{2016}{{Ba\ et~al.}}{{Ba, Kiros, and Hinton}}}
\bibcite{dmitry2016instance}{{91}{2016}{{Dmitry\ et~al.}}{{Dmitry, Andrea, and Victor}}}
\bibcite{wu2018group}{{92}{2018}{{Wu\ et~al.}}{{Wu and He}}}
\bibcite{goodfellow2016deep}{{93}{2016}{{Goodfellow\ et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{koster2017flexpoint}{{94}{2017}{{K{\"o}ster\ et~al.}}{{K{\"o}ster, Webb, Wang, Nassar, Bansal, Constable, Elibol, Gray, Hall, Hornof, et~al.}}}
\bibcite{hu2018hashing}{{95}{2018}{{Hu\ et~al.}}{{Hu, Wang, and Cheng}}}
\bibcite{russakovsky2015imagenet}{{96}{2015}{{Russakovsky\ et~al.}}{{Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, et~al.}}}
\bibcite{li2016ternary}{{97}{2016{}}{{Li\ et~al.}}{{Li and Liu}}}
\bibcite{zhu2016trained}{{98}{2016}{{Zhu\ et~al.}}{{Zhu, Han, Mao, and Dally}}}
\bibcite{wang2017fixed}{{99}{2017}{{Wang\ et~al.}}{{Wang and Cheng}}}
\bibcite{hubara2016binarized}{{100}{2016}{{Hubara\ et~al.}}{{Hubara, Courbariaux, Soudry, El-Yaniv, and Bengio}}}
\bibcite{zhou2016dorefa}{{101}{2016}{{Zhou\ et~al.}}{{Zhou, Wu, Ni, Zhou, Wen, and Zou}}}
\bibcite{cai2017deep}{{102}{2017}{{Cai\ et~al.}}{{Cai, He, Sun, and Vasconcelos}}}
\bibcite{he2014reshaping}{{103}{2014{}}{{He\ et~al.}}{{He, Fan, Qian, Tan, and Yu}}}
\bibcite{srinivas2015data}{{104}{2015}{{Srinivas\ et~al.}}{{Srinivas and Babu}}}
\bibcite{hu2016network}{{105}{2016}{{Hu\ et~al.}}{{Hu, Peng, Tai, and Tang}}}
\bibcite{mariet2015diversity}{{106}{2015}{{Mariet\ et~al.}}{{Mariet and Sra}}}
\bibcite{jaderberg2014speeding}{{107}{2014}{{Jaderberg\ et~al.}}{{Jaderberg, Vedaldi, and Zisserman}}}
\bibcite{chen2014dadiannao}{{108}{2014{}}{{Chen\ et~al.}}{{Chen, Luo, Liu, Zhang, He, Wang, Li, Chen, Xu, Sun, et~al.}}}
\bibcite{liu2016cambricon}{{109}{2016}{{Liu\ et~al.}}{{Liu, Du, Tao, Han, Luo, Xie, Chen, and Chen}}}
\bibcite{srivastava2014dropout}{{110}{2014{}}{{Srivastava\ et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{holi1993finite}{{111}{1993}{{Holi\ et~al.}}{{Holi and Hwang}}}
\bibcite{venkataramani2014axnn}{{112}{2014}{{Venkataramani\ et~al.}}{{Venkataramani, Ranjan, Roy, and Raghunathan}}}
\bibcite{pillai2001real}{{113}{2001}{{Pillai\ et~al.}}{{Pillai and Shin}}}
\bibcite{olshausen1996emergence}{{114}{1996}{{Olshausen\ et~al.}}{{Olshausen and Field}}}
\bibcite{boureau2008sparse}{{115}{2008}{{Boureau\ et~al.}}{{Boureau, Cun, et~al.}}}
\bibcite{lee2008sparse}{{116}{2008}{{Lee\ et~al.}}{{Lee, Ekanadham, and Ng}}}
\bibcite{lee2007efficient}{{117}{2007}{{Lee\ et~al.}}{{Lee, Battle, Raina, and Ng}}}
\bibcite{henneaux1992quantization}{{118}{1992}{{Henneaux\ et~al.}}{{Henneaux and Teitelboim}}}
\bibcite{mackay2003information}{{119}{2003}{{MacKay}}{{}}}
\bibcite{huffman1952method}{{120}{1952}{{Huffman}}{{}}}
\bibcite{witten1987arithmetic}{{121}{1987}{{Witten\ et~al.}}{{Witten, Neal, and Cleary}}}
\bibcite{jbig}{{122}{1993}{{jbi}}{{}}}
\bibcite{lecun1998gradient}{{123}{1998}{{LeCun\ et~al.}}{{LeCun, Bottou, Bengio, and Haffner}}}
\bibcite{Srivastava2014}{{124}{2014{}}{{Srivastava\ et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{krizhevsky2012cuda}{{125}{2012}{{Krizhevsky}}{{}}}
\bibcite{sak2014long}{{126}{2014}{{Sak\ et~al.}}{{Sak, Senior, and Beaufays}}}
\bibcite{muralimanohar2007optimizing}{{127}{2007}{{Muralimanohar\ et~al.}}{{Muralimanohar, Balasubramonian, and Jouppi}}}
\bibcite{duff2002overview}{{128}{2002}{{Duff\ et~al.}}{{Duff, Heroux, and Pozo}}}
\bibcite{yu2017scalpel}{{129}{2017}{{Yu\ et~al.}}{{Yu, Lukefahr, Palframan, Dasika, Das, and Mahlke}}}
\bibcite{hill2017deftnn}{{130}{2017}{{Hill\ et~al.}}{{Hill, Jain, Hill, Zamirai, Hsu, Laurenzano, Mahlke, Tang, and Mars}}}
\bibcite{wen2016learning}{{131}{2016}{{Wen\ et~al.}}{{Wen, Wu, Wang, Chen, and Li}}}
\bibcite{lebedev2016fast}{{132}{2016}{{Lebedev\ et~al.}}{{Lebedev and Lempitsky}}}
\bibcite{li2016pruning}{{133}{2016{}}{{Li\ et~al.}}{{Li, Kadav, Durdanovic, Samet, and Graf}}}
\bibcite{mao2017exploring}{{134}{2017}{{Mao\ et~al.}}{{Mao, Han, Pool, Li, Liu, Wang, and Dally}}}
\FN@pp@footnotehinttrue 
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {chapter}{致谢}{88}{chapter*.52}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\FN@pp@footnotehinttrue 
\@writefile{toc}{\contentsline {chapter}{在读期间发表的学术论文与取得的研究成果}{89}{chapter*.53}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\ttl@finishall
