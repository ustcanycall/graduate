\chapter{总结和展望}

\section{本文工作总结}
人工神经网络近年来在学术界和工业界获得迅猛发展，目前神经网络已经成为众多领域，包括图像识别、物体检测、语音处理和自然语言处理等获得非常理想的效果。神经网络的功能不断增强的过程中，规模不断扩大，层数不断加深，因此神经网络需要庞大的存储资源，计算资源和能耗完成运算。因此研究者采用剪枝的方法将神经网络稀疏化，从而减少神经网络参数和计算量。但是稀疏神经网络的不规则性会使得CPU，GPU和大部分神经网络加速器无法利用稀疏性获得性能提升，尽管有一部分加速器能够支持稀疏特性，但是开销较大，而且效果并不理想。因此本文提出了一种软硬件结合的方法处理稀疏神经网络的不规则性，从算法和架构设计的角度提出多种优化策略，显著提升处理稀疏神经网络的效果。本文的主要创新点体现在一下几个方面：

\textbf{1.软硬件结合的方法处理稀疏神经网路的不规则性。}本文首次提出采用软硬件结合的方法处理稀疏神经网络不规则性。在软件方面我们提出了一种新的神经网络剪枝方法，即粗粒度剪枝，对神经网络进行粗粒度稀疏，从而减少稀疏神经网络的不规则形；在硬件方面，我们提出了Cambricon-S来高效处理粗粒度稀疏和动态神经元稀疏，从而更加有效地利用稀疏性，获得性能的提升。

\textbf{2.基于局部收敛的神经网络压缩算法。}本文通过大量的实验，观察到了局部收敛的现象，即神经网络的权值不是一种随机分布的情况，而是在训练过程中大的权值会聚集成簇。本文根据观察到的局部收敛现象，提出了一种新的神经网络压缩算法。压缩算法包括三个步骤，分别是粗粒度剪枝，局部量化和熵编码。粗粒度剪枝将神经网络的权值分为多个权值块，当一个权值块符合某个条件时将从网络拓扑中被完全剪除，粗粒度剪枝能够显著减少稀疏的神经网络的不规则度。局部量化将权值分为多个子区域，然后在每一个子区域内分别进行量化，从而降低表示权值的比特数，进一步压缩神经网络。之后，我们采用熵编码对神经网络进行无损压缩。新的神经网络压缩算法不仅能够降低稀疏神经网络的稀疏度，获得非常理想的压缩比，还能降低稀疏神经网络的不规则度。粗粒度剪枝可以将稀疏神经网络不规则性平均减少20.13倍。受益于粗粒度稀疏和局部量化，我们新的压缩方法可以将AlexNet和VGG分别压缩79倍和98倍，远远高于目前最先进的两个神经网络压缩算法Deep Compression (35倍/49倍) 和 CNNPack (39倍/46倍)。

\textbf{3.高效处理压缩神经网络的神经网络加速器。}本文设计并提出了首个能够支持粗粒度稀疏神经网络的加速器微结构Cambricon-S。该微结构的主要的特征是一个共享的神经元选择模块NSM，用来挖掘粗粒度权值稀疏的特性，即神经元共享和索引信息共享。同时微结构还包括SSM，Encoder和WDM，分别用来处理动态神经元稀疏，动态压缩神经元和解码经过局部量化的权值。新型加速器不仅能够处理普通的稠密神经网络，还能够通过打开/关闭稀疏处理模块支持多种稀疏/量化情况，包括神经元稀疏，粗粒度权值稀疏，神经元/权值同时稀疏情况，局部量化等。新型加速器能够非常高效的利用稀疏和量化，获得非常理想的性能和非常低的能耗。
同时，为了减少用户的编程负担，我们为加速器设计了专用的基于库的编程模型。编程模型中集成了加速器专用的编译器，能够将C++高级语言编译成为加速器可执行指令；值得注意的，编译器能够静态分析指令之间的依赖关系，提取出没有依赖关系的指令，使它们能够在加速器中同时进行发射。
在65nm工艺下，加速器的总面积和总功耗分别是 $6.82mm^2$ 和 $821.19mW$，工作频率为$1GHz$，吞吐量为$512GOP/s$，片上 SRAM共为$54KB$。实验结果显示，对比目前最先进的稀疏神经网络加速器Cambricon-X，新型加速器能够获得1.71倍的性能提升，同时降低1.75倍的能耗。

\textbf{4.性能优化的模拟器。}本文为神经网络加速器设计了专用的模拟器。优化模拟器在周期精确模拟器的基础上，进行多方面的改进和优化，使其能够在误差允许范围内快速进行性能模拟。优化模拟器采用事件触发进行设计，能够对加速器进行高层次抽象，同时采用了精简的事件集合，并采用建模的方式对模拟事件的执行时间。优化模拟器的性能周期精度模拟器的$41.41$倍，但是误差小于$3\%$。

\section{未来研究展望}

本文提出了软硬件结合的方法处理稀疏神经网络的不规则性。在软件上，粗粒度稀疏减少了神经网络的不规则度，同时新的压缩算法能够深度压缩神经网络，取得非常理想的压缩比。在硬件上，新型加速器能够处理粗粒度稀疏神经网络，相比以前的稀疏神经网络加速器能够进一步提升性能，降低能耗。

未来神经网络处理器的发展主要会聚焦在两个方面，分别是嵌入式系统领域和云端处理领域。

\textbf{高性能，低功耗的神经网络芯片。}
针对嵌入式设备中的神经网络应用，未来的工作可以从两个方面入手。一方面，在算法方面，我们可以通过裁剪网络，量化，低精度计算等方法，在精度损失允许的范围内，减少神经网络的参数数量和计算量，从而使得神经网络的规模能够部署到嵌入式设备中。另一个方面，我们需要设计特定的神经网络处理器，使其能够支持多种神经网络优化算法，如稀疏，量化等，从而获得高性能的同时降低能耗。

\textbf{高吞吐量，低延迟的多核神经网络处理器。}
随着越来越多的神经网络算法部署到云端，云计算（cloud computing）必须能够保证神经网络运算的服务质量（Quality of Service，QoS）的同时，最小化资源消耗。针对神经网络应用的云服务，未来的工作可以从两个方面进行优化。一方面，在底层硬件方面，我们可以设计高吞吐量，低延迟的多核神经网络处理器，并将其部署到云端来代替传统的CPU和GPU，为神经网络应用提供服务。设计多核神经网络处理器会涉及到诸多设计选择（design choice）的问题，如单核处理器的框架的选择，片上网络（network on chip, NoC）的设计（包括核之间如何互联，核与存储器之间如何路由等），存储设计问题（如存储器层次划分，存储器容量，片上缓存选择Cache的形式还是Stratch-pad的形式等）。另一方面，在上层软件支撑方面，我们需要针对神经网络应用的特点和底层硬件的特点，设计针对云端神经网络应用的编程框架。编程框架中的一个核心问题就是调度问题，即如何将神经网络应用部署到底层硬件中，保证应用的QoS需求，同时最小化资源的消耗。


